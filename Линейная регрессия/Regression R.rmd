---
title: "Семинар 8"
author: "Многомерные статистические методы -- 2020-21 гг. -- 3 курс"
date: "13.03.2021"
output:
  html_document:
    df_print: paged
  pdf_document:
    df_print: paged
lang: ru-russian
---



### Загрузка пакетов и данных

```{r}
getwd()
#setwd('/Users/anastasiakoroleva/Учеба/МСМ/')
```

Сегодня нам понадобятся пакеты:
```{r message=FALSE, warning=FALSE}
#install.packages("DescTools")
#install.packages("rio")
#install.packages("robustHD")
#install.packages("ggpubr")
#install.packages("ggplot2")
#install.packages("lmtest")
#install.packages("sjPlot")
#install.packages("normtest")
#install.packages("GGally")
#install.packages("leaps")

library(DescTools)
library(rio)
library(robustHD)
library(ggpubr)
library(ggplot2)
library(lmtest)
library(sjPlot)
library(normtest)
library(GGally)
library(leaps)
```

Данные о ценах на квартиры в Москве, размещенных ВШЭ в открытом доступе на платформе [Kaggle](https://www.kaggle.com/hugoncosta/price-of-flats-in-moscow).

```{r}
data <- import('flats_moscow.xlsx') #в пакете rio
data$price <- data$price * 77 / 10 # теперь зависимая переменная измерена в 10 000 руб.

data$walk <- factor(data$walk) # бинарные переменные теперь считываются как факторы
data$brick <- factor(data$brick)
data$floor <- factor(data$floor)

data <- data[, c('price', 'totsp', 'livesp', 'kitsp', 'dist', 'metrdist', 'walk', 'brick', 'floor')] #пеерчисляем переменные которые хотим оставить

str(data)
```

### Постановка задачи

Набор переменных:

Обозначение переменной | Переменная
------------- | -------------
price | цена квартиры, $ 1 000
totsp | общая площадь квартиры, кв.м.
livesp | жилая площадь квартиры, кв.м.
kitsp | площадь кухни, кв.м.
dist | расстояние от центра в км.
metrdist | расстояние до метро в минутах
walk | 1 – пешком от метро, 0 – на транспорте
brick | 1 – кирпичный, монолит ж/б, 0 – другой
floor | 1 – этаж кроме первого и последнего, 0 – иначе

Возможная гипотеза исследования: цена на квартиру тем выше, чем ближе дом, в котором расположена квартира, к метро (при прочих равных). 

Проверить гипотезу можно на примере рынка квартир в Москве. За близость дома к метро отвечает переменная `metrdist`, расстояние до метро в минутах. Тогда в терминах модели регрессии в пользу тестируемой гипотезы будет говорить отрицательное значение оценки коэффициента при данном регрессоре и ее статистически значимое отличие от нуля.

Будем предполагать, что располагаем данными случайной репрезентативной выборки.

### Обработка данных

В нашей выборке налюдений цен есть экстремальные наблюдения.

Построим ящичковые диаграммы для всех интересуемых переменных. Для сопоставимости следует стандартизировать данные. Но в силу большого числа нетипичных наблюдений воспользуемся робастной стандартизацией. Она аналогична процедуре Z-стандартизации (центрирование и затем нормирование), но использует робастные аналоги оценок средней и среднеквадратического отклонения - медиану и медиану абсолютных отклонений.

```{r}
rdata <- data[, 1:6]
rdata$price <- robStandardize(rdata$price)
rdata$totsp <- robStandardize(rdata$totsp)
rdata$livesp <- robStandardize(rdata$livesp)
rdata$kitsp <- robStandardize(rdata$kitsp)
rdata$dist <- robStandardize(rdata$dist)
rdata$metrdist <- robStandardize(rdata$metrdist)
boxplot(rdata)
```

```{r}
quantile(data$price, probs = c(0.05, 0.95))
sort(boxplot.stats(data$price, coef = 3)$out)
```

Для 95% квартир цена ниже 16 млн руб., при этом в выборке имеются квартиры с ценой от 22 до 56 млн руб. Нас интересует проверка наличия связи между ценой и близостью дома к метро, поэтому, чтобы снизить влияние экстремальных наблюдений на дальнейшие результаты, исключим объекты с такими значениями из выборки. Так как можно достаточно уверенно принять, что расстояние до метро не имеет существенного значения для сегмента элитного жилья.

```{r}
out_of_3IQR <- boxplot.stats(data$price, coef = 3)$out
data <- data[-which(data$price %in% out_of_3IQR),]
str(data)
```

В случае иной постановки задачи исключение объектов из выборки было бы возможно не вполне обоснованно.

### Двумерная модель линейной регрессии

Начнем с элементарного - двумерная модель линейной регрессии:
```{r}
lm1 <- lm(price ~ metrdist, data)
summary(lm1) #intercept = b1
# коэф значимый так как p-value близко к 0
# нулевая гипотеза подтвержается, модель значима
# модель значима, R^2 мал
```

Визуализация:
```{r}
ggscatter(data, x = 'metrdist', y = 'price', cor.coef = TRUE, size = 0.1,
          add = 'reg.line', add.params = list(color = 'blue'), conf.int = TRUE,
          xlab = 'расстояние от дома до метро, мин.', ylab = 'цена квартиры, 10 000 руб.')
```

Еще один способ (если интересует связь цены и площади):
```{r}
ggplot(data, aes(y = price, x = totsp)) + geom_point() + geom_smooth(method='lm') 
```

Можно построить графики для оцененных уравнений двумерной регрессии с каждым регрессором в отдельности, указав в качестве аргумента оценку модели с этими регрессорами:
```{r}
plot_model(lm(price ~ metrdist + livesp + kitsp + dist, data), type = 'slope') #красные прямые
```

### Линейная модель множественной регрессии

Если не включить в модель существенные переменные, то оценки коэффициентов вообще говоря могут оказаться смещенными. Рассмотрим теперь линейную модель множественной регрессии со всеми имеющимися в нашем распоряжении объясняющими переменными:

```{r}
lm2 <- lm(price ~ ., data)
summary(lm2) #коэф R^2 вырос, модель значима, качество модели улучшилось
```

Еще один этап валидации результатов - анализ остатков:
```{r}
res <- lm2$residuals
plot(seq(1, nrow(data), 1), res, pch = 16, xlab = 'номер наблюдения', ylab = 'остаток')
abline(h = mean(res), col = 'red', lwd = 2)

hist(res, breaks = sqrt(length(res)), xlab = 'остаток', ylab = 'частота',
     main = 'Гистограмма распределения частот остатков')

qqnorm(res, pch = 16, xlab = 'теоретический квантиль', ylab = 'выборочный квантиль',
       main = 'Нормальный график квантиль-квантиль')
qqline(res, col = 'red', lwd = 2)
```

```{r}
PearsonTest(res) # нулевая гипотеза - остатки распределены нормально. p-value мало, то есть гипотеза отклоняется
jb.norm.test(res) # 
```

```{r}
ggnostic(lm2)
```

Мы получаем четыре графика для каждого регрессора, отражающие:

1. зависимость величины остатка от значения регрессора (позволяет проверить предпосылку о гомоскедастичности)

2. зависимость оценки стандартного отклонения случайной ошибки от каждого наблюдения значения регрессора - какой была бы оценка, если бы данное наблюдение было отброшено? (позволяет проверить чувствительность результатов к экстремальным наблюдениям)

3. зависимость величины приращения прогноза при увеличении фактического значения зависимой переменной на единицу в данном наблюдении от каждого наблюдения значения регрессора

4. зависимость величины расстояния Кука от каждого наблюдения значения регрессора

Расстояние Кука для каждого наблюдения показывает, в какой степени изменятся модельные значения для всех других объектов, если данное наблюдение отброшено.

Построим график модельных и фактических значений (предсказанных и истинных) зависимой переменной:
```{r}
pred <- predict(lm2)
plot(seq(1, nrow(data), 1), data$price, pch = 16,
     xlab = 'номер наблюдения', ylab = 'цена квартиры, 10 000 руб.')
lines(seq(1, nrow(data), 1), pred, col = 'blue', lwd = 2)
```

```{r}
pred <- predict(lm2)
df_true <- data.frame(x = data$totsp, y = data$price)
df_true$цена <- 'фактическая'

df_pred <- data.frame(x = data$totsp, y = pred)
df_pred$цена <- 'модельная'

df <- rbind.data.frame(df_true, df_pred)

ggplot(df, aes(x = x, y = y)) +
  geom_point(aes(color = цена)) +
  labs(x = 'общая площадь', y = 'цена квартиры, 10 000 руб.')
```

Зависимость предсказанных значений от значений регрессоров:
```{r}
plot_model(lm2, type = 'pred')
```

```{r}
plot_model(lm2, type = 'pred', terms = c('metrdist', 'walk'))
```

Интервальные оценки коэффицентов регрессии:
```{r}
coefci(lm2, level = 0.95)
```

Визуализация интервальных оценок коэффициентов регрессии:
```{r}
plot_model(lm2, ci.lvl = 0.95)
```

### Отбор регрессоров (feature selection)

Иногда бывает так, что результаты применения статистических тестов, в частности t-тестов на значимость отдельных коэффициентов, зависят от набора регрессоров - это одно из возможных последствий проблемы мультиколлинеарности, когда присутствует довольно сильная линейная связь между регрессорами. Прямо сейчас мы попробуем самый простой способ преодоления этой проблемы - пересмотр набора регрессоров.

В прикладной статистике и машинном обучении распространенным способом отбора регрессоров является метод включения и метод исключения.

Идея состоит в следующем: при методе включения сначала строится модель с одним регрессором, для которого парный коэффициент корреляции с зависимой переменной является наибольшим по модулю. Далее добавляется следующий по тесноте линейной связи регрессор, и так до тех пор пока коэффициент при добавленном регрессоре не окажется незначимым.

```{r}
regfit_fwd <- regsubsets(price ~ ., data, force.in = c(6, 7, 8), intercept = TRUE,
                         method = 'forward')

# force.in - т.к. в наборе регрессоров присутствуют дамми-переменные, мы не можем рассчитать коэффициент корреляции с зависимой переменной, поэтому включаем их в модель по умолчанию, а для этого передаем значением этого аргумента вектор номеров этих переменных

# method = 'forward' - метод включения. Если звездочка, то включаем в модель

summary(regfit_fwd)
```

Так как модели с разным числом регрессоров - сравниваем по скорректированному R2:
```{r}
summary(regfit_fwd)$adjr2
```

```{r}
plot(regfit_fwd, scale = 'adjr2') #R^2 значение это adjr2
```

Метод исключения: сначала строится полная модель со всеми регрессорами, далее исключается регрессор с наибольшим p-value и так до тех пор, пока не останутся регрессоры, которым соответствуют значимые коэффициенты.
```{r}
regfit_bwd <- regsubsets(price ~ ., data, force.in = c(6, 7, 8), intercept = TRUE,
                        method = 'backward')
summary(regfit_bwd)
summary(regfit_bwd)$adjr2
```







### Нелинейная модель регрессии

Можно рассмотреть полиномиальную модель, например, квадратическую зависимость цены от площади кухни:
```{r}
ggscatter(data, x = 'kitsp', y = 'price')
```

Тогда можно добавить соответствующий регрессор и оценить линейную модель с этим регрессором:
```{r}
data$kitsp2 <- data$kitsp^2
nlm1 <- lm(price ~ ., data) 
summary(nlm1)
```

Можно рассмотреть экспоненциальную модель. Для этого следует логарифмировать значения зависимой переменной, и затем оценить линейную модель с новыми значениями зависимой переменной:
```{r}
data$l_price <- log(data$price)
nlm2 <- lm(l_price ~ totsp + livesp + kitsp + dist + metrdist + walk + brick + floor, data)
summary(nlm2)
```

Можно рассмотреть степенную модель, но в таком случае придется исключить дамми-переменные. Для этого следует логарифмировать наблюдения всех регрессоров и зависимой переменной (нам повезло, что все переменные определены на положительной полуоси):

```{r}
data$l_totsp <- log(data$totsp)
data$l_livesp <- log(data$livesp)
data$l_kitsp <- log(data$kitsp)
data$l_dist <- log(data$dist)
data$l_metrdist <- log(data$metrdist)
nlm3 <- lm(l_price ~ l_totsp + l_livesp + l_kitsp + l_dist + l_metrdist, data)
summary(nlm3)  
```

Нелинейные модели и линейные можно сравнивать между собой исключительно на основе значений информационных критериев (они основаны на значении функции правдоподобия и штрафуют модель за излишнюю сложность).

Так как значение критериев обратно зависит от значения функции правдоподобия, то чем меньше значение, тем лучше:

```{r}
#Информационный критерий Акаике. Сравниваем с абсолютным значением. Чем меньше тем лучше
AIC(lm1)
AIC(lm2)
AIC(nlm1)
AIC(nlm2)
AIC(nlm3) #Лучшая модель - степенная по этому критерию
```

```{r}
#Информационный критерий Байесовский информационный критерий (Шварца). Сравниваем с абсолютным значением. Чем меньше тем лучше
BIC(lm1)
BIC(lm2)
BIC(nlm1)
BIC(nlm2)
BIC(nlm3) #Лучшая модель - степенная по этому критерию
```

